
# This workflow uses GitHub Releases to store the data file.
# It is fully self-contained and does not use any external services.

name: Monthly Data Refresh via GitHub Releases

on:
  schedule:
    - cron: '30 5 1 * *' # Runs at 05:30 UTC on the 1st of every month
  workflow_dispatch: # Allows manual runs

jobs:
  update-release-asset:
    runs-on: ubuntu-latest

    steps:
    # Step 1: Check out your repository's code
    - name: Checkout repository
      uses: actions/checkout@v4

    # Step 2: Set up Python
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Step 3: Install Python dependencies
    - name: Install dependencies
      run: |
        pip install pandas pyarrow

    # Step 4: Run the data pipeline to create the new Parquet file
    - name: Download and Parse Data
      run: |
        python3 download_script.py --import_name=CDC_StandardizedPrecipitationIndex --config_file=import_configs.json
        python3 parse_precipitation_index.py CDC_StandardizedPrecipitationIndex_input.csv output/CDC_StandardizedPrecipitationIndex_output.csv
      working-directory: ./index

    - name: Convert CSV to Parquet
      run: |
        import pandas as pd
        df = pd.read_csv('index/output/CDC_StandardizedPrecipitationIndex_output.csv')
        df['countyfips'] = df['countyfips'].astype(str).str.zfill(5)
        df.to_parquet('spi_data.parquet', engine='pyarrow')
      shell: python

    # Step 5: Upload the new data file to the GitHub Release
    - name: Upload Release Asset
      uses: actions/upload-release-asset@v1
      with:
        upload_url: https://uploads.github.com/repos/vishalworkdatacommon/climate_dasboard/releases/$(gh release list --limit 1 --json tagName -q '.[0].tagName' | sed 's/v//')/assets
        asset_path: ./spi_data.parquet
        asset_name: spi_data.parquet
        asset_content_type: application/octet-stream
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
